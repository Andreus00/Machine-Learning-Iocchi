\section{Unsupervised Learning}
We want to learn a distribution without knowing the labels. This is considered a more complex and difficult problem than supervised learning.
\begin{equation}
    \begin{multlined}
        f: X \xrightarrow{} Y \\
        D = \left\{ (x_{n}) \right\}
    \end{multlined}
\end{equation}
\subsection{Gaussian Mixture Model}
We assume that the dataset is generated by a probability distribution which is the sum of different Gaussian functions:
\begin{equation}
    P(x) = \sum_{k=1}^{k}\pi_{k}N\left(x; \mu_{k}, \Sigma_{k}\right)
\end{equation}
Where $\pi_{k}$ is the prior probability, $\mu_{k}$ is the mean and $\Sigma_{k}$ is the covariance matrix.\\
When we have these information we can easily generate datapoints by sampling the distribution. We firstly select one of the Gaussians based on the probability $\pi$ and then we sample it.

\subsection{K-means}
Now that we know how to sample from a GMM, we can try to find the means of the distributions.

The K-means algorithm tries to find the K centroids of the K Gaussians.\\
The iterative process of the algorithm is the following:
\begin{enumerate}
    \item Begin with a decision on the number of clusters (k)
    \item Initialize the positions of the means. You may assign the training samples randomly, or systematically as follows
    \begin{itemize}
        \item Take the first k training samples as single element cluster
        \item Assign each of the remaining (N - k) training samples to the cluster with the nearest centroid. After each assignment, recompute the centroid of the new cluster.
    \end{itemize}
\end{enumerate}
The algorithm stops when there are no changes in the assignment.\\
The algorithm converges because:
\begin{itemize}
    \item At each switch in step 2, the distance between the points and the centroids is decreased.
    \item There are only a finite number of partitions that assign data points to k clusters.
\end{itemize}

\textbf{Cons} of K-means:
\begin{itemize}
    \item The number of K must be decided before hand. There are algorithms that tries to find the best K.
    \item Sensitive to initial condition (local optimum) when a few data available. 
    \item Not robust to outliers
    \item the result is a circular cluster shape because it is based on distance.
\end{itemize}
Some improvements to K-means:
\begin{itemize}
    \item Use it only if there are many data available
    \item use median instead of mean
    \item define better distance functions.
\end{itemize}

\subsection{Predict GMM}
A different way of predicting the GMM is by introducing a latent variable $z_{k} \in \left\{0, 1\right\}$., with $z = \left(z_{1}, \dots, z_{k}\right)$ and 1-out-of-K encoding.\\
Let's define
\begin{equation}
    P(z_{k} = 1) = \pi_{k}
\end{equation}
\begin{equation}
    P(z_{k}) = \sum_{k=1}^{K}\pi_{k}^{z_{k}}
\end{equation}
For a given value of $z$:
\begin{equation}
    P(x|z_{k} = 1) = N(x; \mu_{k}, \Sigma_{k})
\end{equation}
thus
\begin{equation}
    P(x|z) = \prod_{k=1}^{K}N(x; \mu_{k}, \Sigma_{k})^{Z_{k}}
\end{equation}
Joint distribution: $P(x, z) = P(x|z)P(z)$ (chain rule).\\
When $z$ are variables with 1-out-of-K encoding and $P(z_{k} = 1) = \pi_{k}$,
\begin{equation}
\label{abc}
    P(x) = \sum_{z}P(z)P(x|z) = \sum_{k=1}^{K}\pi_{k}N(x; \mu_{k}, \Sigma_{k})
\end{equation}
GMM distribution can be seen as the maximization of $P(x, z)$ over variables $z$. By reversing the equation \ref{abc} we can learn the GMM.

Let's define the \textbf{posterior} 
\begin{equation}
    \begin{multlined}
        \gamma(z_{k}) \equiv P(z_{k} = 1 | x) = \frac{P(z_{k} = 1)P(x|z_{k} = 1)}{P(x)} = \\
        \frac{\pi_{k}N(x;\mu_{k}, \Sigma_{k})}{\sum_{j=1}^{K}\pi_{j}N(x;\mu_{j}, \Sigma_{j})}
    \end{multlined}
\end{equation}

\subsection{Expectation Maximization (EM)}
It is a general iterative algorithm based on two steps: \textbf{Expectation} and \textbf{Maximization}.\\
The algorithm computes the maximum likelihood 
\begin{equation}
    \underset{params}{argmax}P(X|params)
\end{equation}

Expectation maximization consists of:
\begin{itemize}
    \item \textbf{E step}: Given $\pi_{k}, \mu_{k}\text{, and } \Sigma_{k}$ compute $\gamma(Z_{nk})$
    \item \textbf{M step}: Given $\gamma(Z_{nk})$, compute $\pi_{k}, \mu_{k}\text{, and } \Sigma_{k}$
\end{itemize}

\subsubsection{EM for GMM}
\begin{equation}
    \begin{multlined}
        \mu_{k} = \frac{1}{N_{k}}\sum_{n=1}^{N}\gamma(z_{nk})x_{n}\\
        \Sigma_{k} = \frac{1}{N_{n}}\sum_{n=1}^{K}\gamma(z_{nk})(x_{n}-\mu_{k})(x_{n}-\mu_{k})^{T}\\
        \pi_{k} = \frac{N_{k}}{N}\text{, with }N_{k}=\sum_{n=1}^{N}\gamma(z_{nk})
    \end{multlined}
\end{equation}

The algorithm for GMM:
\begin{itemize}
    \item Initialize $\pi_{k}^{(0)}, \mu_{k}^{(0)}\text{, and } \Sigma_{k}^{(0)}$
    \item repeat until termination condition:
    \begin{itemize}
        \item \textbf{E step}
        \begin{equation}
            \gamma(z_{nk})^{(t + 1)} = \frac{\pi_{k}^{(t)}N(x;\mu_{k}^{(t)}, \Sigma_{k}^{(t)})}{\sum_{j=1}^{K}\pi_{j}^{(t)}N(x;\mu_{j}^{(t)}, \Sigma_{j}^{(t)})}
        \end{equation}
        \item \textbf{M step}
        \begin{equation}
            \begin{multlined}
                \mu_{k}^{(t + 1)} = \frac{1}{N_{k}}\sum_{n=1}^{N}\gamma(z_{nk})^{(t + 1)}x_{n}\\
                \Sigma_{k}^{(t + 1)} = \frac{1}{N_{n}}\sum_{n=1}^{K}\gamma(z_{nk})^{(t + 1)}(x_{n}-\mu_{k}^{(t + 1)})(x_{n}-\mu_{k}^{(t + 1)})^{T}\\
                \pi_{k}^{(t + 1)} = \frac{N_{k}}{N}\text{, with }N_{k}=\sum_{n=1}^{N}\gamma(z_{nk})^{(t + 1)}
            \end{multlined}
        \end{equation}
    \end{itemize}
\end{itemize}
Notice that if we consider only $\mu$, this is the same as K-means.

\subsubsection{General EM problem}
Given observed data, an unobserved latent variable and a parameterized probability distribution $P(Y|\Theta)$, where $\Theta$ are the parameters

Em in general converges to a local optimum. It provides an estimation for the latent variables.

EM has many uses in Unsupervised clustering, Bayesian Networks and Hidden Markov Models.

The method is the following: Given a likelihood function $Q(\Theta'|\Theta)$ which calculates $Y = X \cup Z$, the algorithm is:
\begin{enumerate}
    \item \textbf{Estimation step}: Calculate $Q(\Theta'|\Theta)$ using current hypothesis $\Theta$ and observed data $X$ to estimate probability distribution over $Y$
    \begin{equation}
        Q(\Theta'|\Theta) \xleftarrow{} E[\ln P(Y|\Theta')|\Theta, X]
    \end{equation}
    \item \textbf{Maximization step}: Replace hypothesis $\Theta$ by the hypothesis $\Theta'$ that maximizes the $Q$ function
    \begin{equation}
        \Theta \xleftarrow{} \underset{\Theta'}{argmax} Q(\Theta'|\Theta)
    \end{equation}
\end{enumerate}


